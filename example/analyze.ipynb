{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e529da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_data(folder_path, dataset_name=None, strategy=None, augmentation=None):\n",
    "    \"\"\"\n",
    "    Parse test data from a folder containing log_test_*.csv files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import re\n",
    "    \n",
    "    folder_path = Path(folder_path)\n",
    "    \n",
    "    if not folder_path.exists():\n",
    "        print(f\"❌ Folder does not exist: {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📂 Processing test folder: {folder_path}\")\n",
    "    \n",
    "    prec1_data = []\n",
    "    \n",
    "    # Get all test files\n",
    "    test_files = [f for f in os.listdir(folder_path) \n",
    "                  if f.endswith('.csv') and f.startswith('log_test')]\n",
    "    \n",
    "    if not test_files:\n",
    "        print(f\"  ⚠️ No test files found in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    # If augmentation not provided, extract from folder name or files\n",
    "    if augmentation is None:\n",
    "        if folder_path.name.startswith('test_'):\n",
    "            augmentation = folder_path.name.replace('test_', '')\n",
    "        else:\n",
    "            # Try to extract from first file\n",
    "            if test_files:\n",
    "                first_file = test_files[0]\n",
    "                parts = first_file.replace('log_test_', '').replace('.csv', '').split('_')\n",
    "                augmentation = parts[0] if parts else 'unknown'\n",
    "    \n",
    "    print(f\"  🎨 Augmentation: {augmentation}\")\n",
    "    print(f\"  📊 Found {len(test_files)} test files\")\n",
    "    \n",
    "    for file in test_files:\n",
    "        file_path = folder_path / file\n",
    "        max_prec1 = None\n",
    "        \n",
    "        print(f\"    📄 Processing: {file}\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    prec1_match = re.search(r\"Prec@1\\s+([\\d.]+)\", line)\n",
    "                    if prec1_match:\n",
    "                        prec1 = float(prec1_match.group(1))\n",
    "                        if max_prec1 is None or prec1 > max_prec1:\n",
    "                            max_prec1 = prec1\n",
    "            \n",
    "            if max_prec1 is not None:\n",
    "                prec1_data.append({\n",
    "                    'file': file,\n",
    "                    'dataset': dataset_name or 'unknown',\n",
    "                    'strategy': strategy or 'unknown', \n",
    "                    'augmentation': augmentation,\n",
    "                    'max_Prec@1': max_prec1\n",
    "                })\n",
    "                print(f\"      ✅ Max Prec@1: {max_prec1}\")\n",
    "            else:\n",
    "                print(f\"      ⚠️ No Prec@1 found in {file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"      ❌ Error processing {file}: {e}\")\n",
    "    \n",
    "    # Create and save DataFrame\n",
    "    if prec1_data:\n",
    "        df = pd.DataFrame(prec1_data)\n",
    "        \n",
    "        # Create output filename\n",
    "        if dataset_name and strategy and augmentation:\n",
    "            output_filename = f\"{dataset_name}_{strategy}_{augmentation}_TEST.csv\"\n",
    "        else:\n",
    "            output_filename = f\"test_summary_{augmentation}.csv\"\n",
    "        \n",
    "        output_path = folder_path / output_filename\n",
    "        df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"  ✅ Saved: {output_filename}\")\n",
    "        print(f\"  📈 Summary: {len(prec1_data)} files processed\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(f\"  ⚠️ No valid test results found\")\n",
    "        return None\n",
    "\n",
    "def find_test_folders(root_dir):\n",
    "    \"\"\"\n",
    "    Find test folders in two cases:\n",
    "    1. Direct subdirectories starting with 'test_'\n",
    "    2. Test folders inside strategy/mixed/ subdirectories  \n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    \n",
    "    if not root_dir.exists():\n",
    "        print(f\"❌ Root directory does not exist: {root_dir}\")\n",
    "        return\n",
    "    \n",
    "    dataset_name = root_dir.name.replace('log_', '')\n",
    "    print(f\"\\n🔍 Searching for test folders in: {root_dir} (dataset: {dataset_name})\")\n",
    "    \n",
    "    processed_folders = []\n",
    "    \n",
    "    for subdir in os.listdir(root_dir):\n",
    "        subdir_path = root_dir / subdir\n",
    "        \n",
    "        if not subdir_path.is_dir():\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n📂 Checking subdirectory: {subdir}\")\n",
    "        \n",
    "        # Case 1: Direct test folders (test_*)\n",
    "        if subdir.startswith('test_'):\n",
    "            print(f\"  ✅ Found direct test folder: {subdir}\")\n",
    "            augmentation = subdir.replace('test_', '')\n",
    "            \n",
    "            result_df = parse_data(\n",
    "                subdir_path, \n",
    "                dataset_name=dataset_name,\n",
    "                strategy='unknown',  # Can't determine strategy from this structure\n",
    "                augmentation=augmentation\n",
    "            )\n",
    "            \n",
    "            if result_df is not None:\n",
    "                processed_folders.append(subdir_path)\n",
    "        \n",
    "        else:\n",
    "            # Case 2: Check for mixed folder inside strategy folder\n",
    "            mixed_folder_path = subdir_path / 'mixed'\n",
    "            \n",
    "            if mixed_folder_path.exists():\n",
    "                print(f\"  📁 Found mixed folder: {subdir}/mixed\")\n",
    "                strategy = subdir\n",
    "                \n",
    "                # Look for test files to determine augmentations\n",
    "                test_files = list(mixed_folder_path.glob(\"log_test_*.csv\"))\n",
    "                \n",
    "                if test_files:\n",
    "                    # Extract unique augmentations from test files\n",
    "                    augmentations = set()\n",
    "                    for test_file in test_files:\n",
    "                        file_name = test_file.name\n",
    "                        remainder = file_name.replace('log_test_', '').replace('.csv', '')\n",
    "                        parts = remainder.split('_')\n",
    "                        if parts:\n",
    "                            augmentations.add(parts[0])\n",
    "                    \n",
    "                    print(f\"    🎨 Found augmentations: {list(augmentations)}\")\n",
    "                    \n",
    "                    # Process each augmentation separately\n",
    "                    for augmentation in augmentations:\n",
    "                        print(f\"\\n    🔄 Processing {strategy}/{augmentation}\")\n",
    "                        \n",
    "                        result_df = parse_data(\n",
    "                            mixed_folder_path,\n",
    "                            dataset_name=dataset_name,\n",
    "                            strategy=strategy,\n",
    "                            augmentation=augmentation\n",
    "                        )\n",
    "                        \n",
    "                        if result_df is not None:\n",
    "                            processed_folders.append(mixed_folder_path)\n",
    "                else:\n",
    "                    print(f\"    ⚠️ No test files found in mixed folder\")\n",
    "            \n",
    "            # Case 3: Check for organized test folders within strategy\n",
    "            for item in os.listdir(subdir_path):\n",
    "                item_path = subdir_path / item\n",
    "                if item_path.is_dir() and item.startswith('test_'):\n",
    "                    print(f\"  ✅ Found organized test folder: {subdir}/{item}\")\n",
    "                    strategy = subdir\n",
    "                    augmentation = item.replace('test_', '')\n",
    "                    \n",
    "                    result_df = parse_data(\n",
    "                        item_path,\n",
    "                        dataset_name=dataset_name,\n",
    "                        strategy=strategy,\n",
    "                        augmentation=augmentation\n",
    "                    )\n",
    "                    \n",
    "                    if result_df is not None:\n",
    "                        processed_folders.append(item_path)\n",
    "    \n",
    "    print(f\"\\n🎉 Processed {len(processed_folders)} test folder(s) for {dataset_name}\")\n",
    "    return processed_folders\n",
    "\n",
    "# Main execution\n",
    "def process_all_test_folders():\n",
    "    \"\"\"Process all test folders across target directories\"\"\"\n",
    "    target_dirs = ['log_cinic10', 'log_tiny200']\n",
    "    \n",
    "    print(\"🔍 PROCESSING ALL TEST FOLDERS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_processed = []\n",
    "    \n",
    "    for target_dir in target_dirs:\n",
    "        if not os.path.exists(target_dir):\n",
    "            print(f\"❌ Directory not found: {target_dir}\")\n",
    "            continue\n",
    "            \n",
    "        processed_folders = find_test_folders(target_dir)\n",
    "        all_processed.extend(processed_folders)\n",
    "    \n",
    "    print(f\"\\n🎉 COMPLETE! Processed {len(all_processed)} test folders total\")\n",
    "\n",
    "# Execute\n",
    "process_all_test_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a08e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_all_test_log(root_dir):\n",
    "    \"\"\"\n",
    "    Analyze all test log CSV files and add mean/std statistics\n",
    "    Handles both direct test folders and strategy-based structure\n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    \n",
    "    if not root_dir.exists():\n",
    "        print(f\"❌ Root directory does not exist: {root_dir}\")\n",
    "        return\n",
    "    \n",
    "def analyze_test_file(file_path):\n",
    "    \"\"\"Helper function to analyze a single test CSV file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'max_Prec@1' not in df.columns:\n",
    "            print(f\"      ⚠️ No 'max_Prec@1' column in {file_path.name}\")\n",
    "            return\n",
    "        \n",
    "        results = df['max_Prec@1'].values\n",
    "        mean = np.mean(results)\n",
    "        std = np.std(results, ddof=1)  # Sample standard deviation\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Update the DataFrame with mean and std as new columns\n",
    "        df['mean_Prec@1'] = mean\n",
    "        df['std_Prec@1'] = std\n",
    "        \n",
    "        # Save updated file\n",
    "        df.to_csv(file_path, index=False)\n",
    "      \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ Error analyzing {file_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfe4b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_test_file('/home/hamt/light_weight/imbalanced-DL/example/log_cinic10/ERM/test_None/cinic10_ERM_None_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5fe101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "def convert_args_to_dict(dir_path):\n",
    "    for file in os.listdir(dir_path):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(dir_path, file), 'r') as f:\n",
    "                content = f.read().strip()\n",
    "\n",
    "                # Remove outer braces { }\n",
    "                if content.startswith(\"{\") and content.endswith(\"}\"):\n",
    "                    content = content[1:-1]\n",
    "\n",
    "                # Remove Namespace( ... )\n",
    "                if content.startswith(\"Namespace(\") and content.endswith(\")\"):\n",
    "                    content = content[len(\"Namespace(\"):-1]\n",
    "\n",
    "                # Convert key=value → 'key': value\n",
    "                content = re.sub(r\"(\\w+)=([^,]+)\", r\"'\\1': \\2\", content)\n",
    "\n",
    "                # Wrap with { } so it's a valid Python dict\n",
    "                content = \"{\" + content + \"}\"\n",
    "                try:\n",
    "                    return ast.literal_eval(content)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to parse {file}: {e}\")\n",
    "                    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e007eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Processing: tiny200_exp_0.01_ERM_200_1448/log_test_randaugment.csv\n",
      "    ✅ Max result: 36.06\n",
      "Failed to parse args.txt: invalid syntax (<unknown>, line 1)\n",
      "⚠️ Could not parse arguments for tiny200_exp_0.01_ERM_200_1916\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_2618/log_test_cutout.csv\n",
      "    ✅ Max result: 33.08\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_2916/log_test_None.csv\n",
      "    ✅ Max result: 34.44\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_2591/log_test_autoaugment_imagenet.csv\n",
      "    ✅ Max result: 34.25\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_3448/log_test_randaugment.csv\n",
      "    ✅ Max result: 35.4\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_3618/log_test_cutout.csv\n",
      "    ✅ Max result: 32.57\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_3916/log_test_None.csv\n",
      "    ✅ Max result: 33.91\n",
      "Failed to parse args.txt: invalid syntax (<unknown>, line 1)\n",
      "⚠️ Could not parse arguments for tiny200_exp_0.01_ERM_200_1618\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_1591/log_test_autoaugment_imagenet.csv\n",
      "    ✅ Max result: 34.1\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_2448/log_test_randaugment.csv\n",
      "    ✅ Max result: 35.22\n",
      "📄 Processing: tiny200_exp_0.01_ERM_200_3591/log_test_autoaugment_imagenet.csv\n",
      "    ✅ Max result: 33.51\n",
      "\n",
      "🎉 PROCESSING COMPLETE!\n",
      "📊 Found 10 files\n",
      "\n",
      "📋 RESULTS SUMMARY:\n",
      "                                file                         subdir  dataset  \\\n",
      "0           log_test_randaugment.csv  tiny200_exp_0.01_ERM_200_1448  tiny200   \n",
      "1                log_test_cutout.csv  tiny200_exp_0.01_ERM_200_2618  tiny200   \n",
      "2                  log_test_None.csv  tiny200_exp_0.01_ERM_200_2916  tiny200   \n",
      "3  log_test_autoaugment_imagenet.csv  tiny200_exp_0.01_ERM_200_2591  tiny200   \n",
      "4           log_test_randaugment.csv  tiny200_exp_0.01_ERM_200_3448  tiny200   \n",
      "5                log_test_cutout.csv  tiny200_exp_0.01_ERM_200_3618  tiny200   \n",
      "6                  log_test_None.csv  tiny200_exp_0.01_ERM_200_3916  tiny200   \n",
      "7  log_test_autoaugment_imagenet.csv  tiny200_exp_0.01_ERM_200_1591  tiny200   \n",
      "8           log_test_randaugment.csv  tiny200_exp_0.01_ERM_200_2448  tiny200   \n",
      "9  log_test_autoaugment_imagenet.csv  tiny200_exp_0.01_ERM_200_3591  tiny200   \n",
      "\n",
      "  strategy          augmentation  max_training_result  \n",
      "0      ERM           randaugment                36.06  \n",
      "1      ERM                cutout                33.08  \n",
      "2      ERM                  None                34.44  \n",
      "3      ERM  autoaugment_imagenet                34.25  \n",
      "4      ERM           randaugment                35.40  \n",
      "5      ERM                cutout                32.57  \n",
      "6      ERM                  None                33.91  \n",
      "7      ERM  autoaugment_imagenet                34.10  \n",
      "8      ERM           randaugment                35.22  \n",
      "9      ERM  autoaugment_imagenet                33.51  \n",
      "✅ Results saved to latex/tiny200/ERM/ERM_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "root_dir = 'log_tiny200' \n",
    "results = []\n",
    "\n",
    "for subdir in os.listdir(root_dir):\n",
    "    parts = subdir.split('_')\n",
    "    if len(parts) > 3 and parts[3] == 'ERM':\n",
    "        arguments = convert_args_to_dict(os.path.join(root_dir, subdir))\n",
    "        \n",
    "        if arguments is None:\n",
    "            print(f\"⚠️ Could not parse arguments for {subdir}\")\n",
    "            continue\n",
    "            \n",
    "        data_augment = arguments.get('data_augment', 'unknown')\n",
    "        \n",
    "        for file in os.listdir(os.path.join(root_dir, subdir)):\n",
    "            if file.endswith('.csv') and file.startswith('log_test'):\n",
    "                max_training_result = None  # Reset for each file\n",
    "                \n",
    "                print(f\"📄 Processing: {subdir}/{file}\")\n",
    "                \n",
    "                try:\n",
    "                    with open(os.path.join(root_dir, subdir, file), 'r') as f:\n",
    "                        for line in f:\n",
    "                            # Look for \"Best Prec@1\" results in log_test files\n",
    "                            train_match = re.search(r\"Best Prec@1:\\s+([\\d.]+)\", line)\n",
    "\n",
    "                            if train_match:\n",
    "                                value = float(train_match.group(1))\n",
    "                                if (max_training_result is None) or (value > max_training_result):\n",
    "                                    max_training_result = value\n",
    "                    \n",
    "                    # Add result for this file\n",
    "                    results.append({\n",
    "                        \"file\": file,\n",
    "                        \"subdir\": subdir,\n",
    "                        \"dataset\": root_dir.split('_')[1],\n",
    "                        \"strategy\": subdir.split('_')[3],\n",
    "                        \"augmentation\": data_augment,\n",
    "                        \"max_training_result\": max_training_result,\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"    ✅ Max result: {max_training_result}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ❌ Error processing {file}: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 PROCESSING COMPLETE!\")\n",
    "print(f\"📊 Found {len(results)} files\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n📋 RESULTS SUMMARY:\")\n",
    "print(df)\n",
    "strategy = df['strategy'].iloc[0]\n",
    "dataset = root_dir.split('_')[1]\n",
    "\n",
    "os.makedirs(f'latex/{dataset}/{strategy}', exist_ok=True)\n",
    "# Save results\n",
    "df.to_csv(f'latex/{dataset}/{strategy}/{strategy}_results.csv', index=False)\n",
    "print(f\"✅ Results saved to latex/{dataset}/{strategy}/{strategy}_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d0c03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   mean    std  count    min    max\n",
      "augmentation                                       \n",
      "autoaugment_svhn  33.62   0.25      3  33.33  33.77\n",
      "cutout            23.93  15.61      3   5.91  33.09\n",
      "randaugment       36.26   0.26      3  36.09  36.56\n",
      "NaN               33.90   0.41      3  33.59  34.37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv('/home/hamt/light_weight/imbalanced-DL/example/latex/tiny200/DRW/DRW_results.csv')\n",
    "stats = df.groupby('augmentation',dropna=False)['max_training_result'].agg([\n",
    "    'mean',\n",
    "    'std',\n",
    "    'count',\n",
    "    'min',\n",
    "    'max'\n",
    "]).round(2)\n",
    "print(stats)\n",
    "stats.to_csv(\"/home/hamt/light_weight/imbalanced-DL/example/latex/tiny200/ERM/ERM_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
